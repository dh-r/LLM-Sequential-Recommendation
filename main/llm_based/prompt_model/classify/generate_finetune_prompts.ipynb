{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6e5b1a",
   "metadata": {
    "papermill": {
     "duration": 0.00861,
     "end_time": "2023-04-15T18:56:46.284277",
     "exception": false,
     "start_time": "2023-04-15T18:56:46.275667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate prompts for the variant `LLMSeqPromptClassify` (Section 4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd0d07",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.156373,
     "end_time": "2023-04-15T18:56:49.476637",
     "exception": false,
     "start_time": "2023-04-15T18:56:46.320264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from main.data.session_dataset import SessionDataset\n",
    "from main.popularity.session import SessionBasedPopular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8dcac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 18.223049,
     "end_time": "2023-04-15T18:57:08.456366",
     "exception": false,
     "start_time": "2023-04-15T18:56:50.233317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET: Literal[\"beauty\", \"steam\"] = \"beauty\"\n",
    "\n",
    "# Name of the pickle with the test data for Beauty.\n",
    "TEST_DATA_PICKLE_NAME = f\"{DATASET}_dataset.pickle\"\n",
    "\n",
    "ITEM_NAMES_DF = f\"{DATASET}_item_names.pickle\"\n",
    "\n",
    "EMBEDDINGS_NAME =  f\"embeddings_{DATASET}.csv.gz\"\n",
    "\n",
    "NUM_CLUSTERS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecd3ad-c39d-4843-9f6a-dcd51dd18552",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: SessionDataset = SessionDataset.from_pickle(open(TEST_DATA_PICKLE_NAME, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a3d2b-d854-4a8b-a54b-5f665c9958e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_embeddings = pd.read_csv(\n",
    "    EMBEDDINGS_NAME, compression=\"gzip\"\n",
    ")\n",
    "product_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1f092-8ddf-48b2-ad4c-23c870940d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_id_to_name = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"name\"]\n",
    ")\n",
    "product_name_to_id = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"name\")\n",
    "    .to_dict()[\"ItemId\"]\n",
    ")\n",
    "product_index_to_embedding = (\n",
    "    product_embeddings[[\"ItemId\", \"embedding\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"embedding\"]\n",
    ")\n",
    "product_index_to_embedding = {\n",
    "    k: np.array(json.loads(v)) for k, v in product_index_to_embedding.items()\n",
    "}\n",
    "product_index_to_embedding = np.array(list(product_index_to_embedding.values()))\n",
    "product_index_to_id = list(product_id_to_name.keys())\n",
    "product_id_to_index = {idx: i for i, idx in enumerate(product_index_to_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c816df-97c1-4463-8c19-a66a29025305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_names = list(product_name_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe56da5-555d-46ab-b98d-b022e6d6f98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=0, n_init=\"auto\")\n",
    "clustering = kmeans.fit_predict(product_index_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1cfe4-e51b-49ab-9efb-5a4728b17b8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_product_id_to_cluster = {}\n",
    "counter_per_cluster = {}\n",
    "for cluster in list(set(clustering)):\n",
    "    counter_per_cluster[cluster] = 0\n",
    "    \n",
    "for i, cluster in enumerate(clustering):\n",
    "    counter_per_cluster[cluster] += 1\n",
    "    global_product_id_to_cluster[product_index_to_id[i]] = cluster\n",
    "counter_per_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c8003",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2818.836833,
     "end_time": "2023-04-15T19:44:07.311860",
     "exception": false,
     "start_time": "2023-04-15T18:57:08.475027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model: SessionBasedPopular = SessionBasedPopular()\n",
    "model.train(dataset.get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d44285-3f24-47e0-9529-456af981cbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_to_popular_item = {}\n",
    "for cluster in range(num_clusters):\n",
    "    # Iterate through items from most to least popular\n",
    "    for item in model.items:\n",
    "        if global_product_id_to_cluster[item] == cluster:\n",
    "            # Some products are NaN, we skip those\n",
    "            if not isinstance(product_id_to_name[item], str) and math.isnan(product_id_to_name[item]):\n",
    "                continue\n",
    "            cluster_to_popular_item[cluster] = item\n",
    "            break\n",
    "            \n",
    "cluster_to_popular_item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fe684-4f56-4a20-a830-a3bfd4473661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_df = pd.read_csv(ITEM_NAMES_DF, usecols=[\"ItemId\", \"name\"])\n",
    "item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda819ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_df[item_df['name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67278e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unnamed_item_ids = beauty_product_df[beauty_product_df['name'].isna()]\\\n",
    "    ['ItemId'].unique()\n",
    "unnamed_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530d91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sessions = dataset.get_train_data().groupby('SessionId')\n",
    "\n",
    "train_prompts = {}\n",
    "train_ground_truths = {}\n",
    "\n",
    "# For each session in the train data\n",
    "for session_id, session_data in sessions:\n",
    "    items = session_data['ItemId'].to_numpy()\n",
    "    \n",
    "    # Remove sessions completely when they have an unnamed product\n",
    "    if np.any(np.isin(items, unnamed_item_ids)):\n",
    "        continue\n",
    "        \n",
    "    # Split the train prompt into a 'prompt' and 'ground-truth'.\n",
    "    # In a session of 'n' items, the first 'n-1' items are the prompt and the 'n'th' item is the ground truth. \n",
    "    train_prompts[session_id] = items[:-1]\n",
    "    train_ground_truths[session_id] = items[-1:]\n",
    "\n",
    "len(train_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c123b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_id_to_name = item_df.set_index('ItemId')\\\n",
    "    ['name'].to_dict()\n",
    "product_id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132460b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "textified_train_prompts = {}\n",
    "\n",
    "for session, rec_items in train_prompts.items():\n",
    "    textified_train_prompts[session] = [\n",
    "        product_id_to_name[product_id] for product_id in rec_items\n",
    "    ]\n",
    "\n",
    "textified_train_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536948",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "textified_categories = {}\n",
    "product_id_to_index = {}\n",
    "\n",
    "# Map global product id to index in embedding matrix\n",
    "for i, index in enumerate(product_index_to_id):\n",
    "    product_id_to_index[index] = i\n",
    "\n",
    "for session, ground_truth in train_ground_truths.items():\n",
    "    # Get embedding of ground truth\n",
    "    # Sort distance to kmeans.cluster_centers_\n",
    "    # Check if number 1 cluster corresponds to the cluster that was predicted for this ground truths (in clustering variable)\n",
    "    # Map top-20 clusters to their corresponding product (cluster-to-product)\n",
    "    ground_truth = ground_truth[0]\n",
    "    \n",
    "    gt_embedding = product_index_to_embedding[product_id_to_index[ground_truth]]\n",
    "    gt_embedding = np.array([gt_embedding], dtype=np.float64)    \n",
    "    predictions = euclidean_distances(kmeans.cluster_centers_, gt_embedding).T[0]\n",
    "    nearest_clusters = predictions.argsort()[:TOP_K]\n",
    "    \n",
    "    textified_categories[session] = [product_id_to_name[cluster_to_popular_item[nn]] for nn in nearest_clusters]\n",
    "    \n",
    "    if nearest_clusters[0] != global_product_id_to_cluster[ground_truth]:\n",
    "        break\n",
    "    # textified_recommendations[session] = [\n",
    "    #     product_id_to_name[product_id] for product_id in rec_items\n",
    "    # ]\n",
    "textified_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba323e25-c2e9-4415-af45-866442290229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_categories = [product_id_to_name[item] for item in cluster_to_popular_item.values()]\n",
    "all_categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ddef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a recommender system assistant. You have access to the user's previous purchases and a list of availabe products.\n",
    "Provide 20 product recommendations for this user, only select from the available products.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "The user's previous purchases: \n",
    "{user_item_list}\n",
    "\n",
    "Available products:\n",
    "{potential_recommendation_categories}\n",
    "\n",
    "\n",
    "Please remember to only select recommendations from the available products.\n",
    "\"\"\"\n",
    "\n",
    "ASSISTANT_PROMPT_TEMPLATE = \"\"\"{top_recommendation_categories}\"\"\"\n",
    "\n",
    "def stringify_ranked_list(list_of_items):\n",
    "    stringified_ranked_list = \"\"\n",
    "    for i, item in enumerate(list_of_items, 1):\n",
    "        stringified_ranked_list += f\"{i}. {item}\\n\"\n",
    "    return stringified_ranked_list\n",
    "\n",
    "\n",
    "def create_prompt(train_prompt, recommendation_categories, top_recommendation_categories):\n",
    "    prompt = {}\n",
    "    prompt['messages'] = []\n",
    "    prompt['messages'].append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    })\n",
    "    prompt['messages'].append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": USER_PROMPT_TEMPLATE.format(\n",
    "            user_item_list='\\n'.join(train_prompt),\n",
    "            potential_recommendation_categories='\\n'.join(recommendation_categories)\n",
    "        )\n",
    "    })\n",
    "    prompt['messages'].append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ASSISTANT_PROMPT_TEMPLATE.format(\n",
    "            top_recommendation_categories=stringify_ranked_list(top_recommendation_categories)\n",
    "        )\n",
    "    })\n",
    "\n",
    "    return prompt\n",
    "create_prompt(train_prompt=textified_train_prompts[0],\n",
    "              recommendation_categories=all_categories,\n",
    "              top_recommendation_categories=textified_categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b517b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_session_length = 2\n",
    "num_tokens = 0\n",
    "num_validation_cases = int(0.2 * len(train_prompts))\n",
    "tokens_per_message = 3\n",
    "tokens_per_name = 1\n",
    "\n",
    "train_cases = []\n",
    "validation_cases = []\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Shuffle session ids to get a random validation set.\n",
    "random.seed(42)\n",
    "session_ids = list(train_prompts.keys())\n",
    "random.shuffle(session_ids)\n",
    "\n",
    "for i, session_id in enumerate(session_ids): \n",
    "    train_session = train_prompts[session_id]\n",
    "\n",
    "    # We skip sessions that are too short.\n",
    "    if len(train_session) < min_session_length:\n",
    "        continue\n",
    "    \n",
    "    # Create prompt\n",
    "    train_prompt = textified_train_prompts[session_id]\n",
    "    session_recommendations = textified_categories[session_id]\n",
    "    prompt = create_prompt(\n",
    "        train_prompt=train_prompt,\n",
    "        recommendation_categories=all_categories,\n",
    "        top_recommendation_categories=session_recommendations\n",
    "    )\n",
    "\n",
    "    # We skip sessions that are too long.\n",
    "    num_prompt_tokens = 0\n",
    "    for message in prompt['messages']:\n",
    "        num_prompt_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_prompt_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_prompt_tokens += tokens_per_name\n",
    "    num_prompt_tokens += 3  # every reply is primed \n",
    "                            # with <|start|>assistant<|message|>\n",
    "    \n",
    "    # if num_prompt_tokens > 4096:\n",
    "    #     continue\n",
    "\n",
    "    # Add to validation or training set.\n",
    "    if i < num_validation_cases: \n",
    "        validation_cases.append(prompt)\n",
    "    else: \n",
    "        num_tokens += num_prompt_tokens\n",
    "        train_cases.append(prompt)\n",
    "\n",
    "# Convert training to JSONL.\n",
    "train_cases = [\n",
    "    json.dumps(train_case) \n",
    "    for train_case in train_cases\n",
    "]\n",
    "train_string = '\\n'.join(train_cases)\n",
    "\n",
    "# Convert validation to JSONL.\n",
    "validation_cases = [\n",
    "    json.dumps(validation_case) \n",
    "    for validation_case in validation_cases\n",
    "]\n",
    "validation_string = '\\n'.join(validation_cases)\n",
    "\n",
    "with open(\"train_cases_llmseqprompt_classify.jsonl\", \"w\") as f:\n",
    "    f.write(train_string) \n",
    "\n",
    "with open(\"validation_cases_llmseqprompt_classify.jsonl\", \"w\") as f:\n",
    "    f.write(validation_string) \n",
    "\n",
    "print(f'Training cases: {len(train_cases)}')\n",
    "print(f'Validation cases: {len(validation_cases)}')\n",
    "print(f\"Num tokens: {num_tokens}\")\n",
    "cost = num_tokens * (0.008 / 1000)\n",
    "print(f\"Costs to train GPT-3 turbo one epoch, roughly: ${cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca1e4d-2600-48f6-a2b4-dea86514b0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"cluster_to_popular_item.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(cluster_to_popular_item, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e9d97-eee8-4fff-8a15-8b27813ba650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"global_product_id_to_cluster.json\", \"wb\") as outfile:\n",
    "    json.dump(global_product_id_to_cluster, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2909.815931,
   "end_time": "2023-04-15T19:45:15.481286",
   "environment_variables": {},
   "exception": null,
   "input_path": "model_bert.ipynb",
   "output_path": "model_bert.ipynb",
   "parameters": {
    "CORES": 12,
    "HYPERSEARCH": false,
    "IS_VERBOSE": true,
    "TOP_K": 20,
    "WORKING_DIR": "FP_SG/qcommerce/12-04-2023_one_month/session"
   },
   "start_time": "2023-04-15T18:56:45.665355",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f788e71502201775ebe5fd014c14b82df146d1d4385a1e3ba9d4321db0a70aa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
