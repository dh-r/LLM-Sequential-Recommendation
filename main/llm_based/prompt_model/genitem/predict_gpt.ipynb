{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d438c1-7c21-431b-b521-2ee4532bb0a9",
   "metadata": {},
   "source": [
    "# Predict `GenItem` variant for Beauty or Steam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7119fd-4203-4a78-9595-ee29aebb6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "from typing import Literal\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a33a6e-1c7c-4533-8e2b-44c2eec9ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in name of finetuned_model.\n",
    "MODEL_NAME = \"\"\n",
    "\n",
    "DATASET: Literal[\"beauty\", \"steam\"] = \"beauty\"\n",
    "\n",
    "# Name of the pickle with the test data for Beauty.\n",
    "TEST_DATA_PICKLE_NAME = f\"test_data_{DATASET}.pickle\"\n",
    "\n",
    "# Name of the embeddings DF for \n",
    "EMBEDDINGS_NAME = f\"embeddings_{DATASET}.csv.gz\"\n",
    "\n",
    "# Fill in OpenAI key\n",
    "OPENAI_KEY = \"\"\n",
    "\n",
    "# Hyperparameters\n",
    "TOP_K = 20\n",
    "TEMPERATURE = 0\n",
    "TOP_P = 1.0\n",
    "\n",
    "# Correspond to respectively 4.1 to 4.4\n",
    "VARIANT: Literal[\"genitem\", \"genlist\", \"class\", \"rank\"] = \"genitem\"\n",
    "TOTAL_MODEL_NAME = f\"{MODEL_NAME}_{VARIANT}_temp_{TEMPERATURE}_top_p_{TOP_P}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63dd13-b8d4-4f93-a6d5-b38f5d6d436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genitem(completion: str) -> str:\n",
    "    return completion.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc903f4e-c479-4e6f-bdda-f118f0e9d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \n",
    "\"\"\"Provide a unique item recommendation that is complementary to the user's item list. \n",
    "Ensure the recommendation is from items included in the data you are fine-tuned with. List only the item name.\n",
    "\"\"\",\n",
    "    }\n",
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"The user's item list:\\n{ITEMS}\",\n",
    "}\n",
    "parse_method = parse_genitem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b2d9b-7889-43ec-ac27-735abb77d5eb",
   "metadata": {},
   "source": [
    "## Load test prompts\n",
    "\n",
    "We expect a pickle in the form:\n",
    "```\n",
    "{\n",
    "    SESSION_ID : TEST_PROMPTS, TEST_GROUND_TRUTHS,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "For example:\n",
    "```\n",
    "{\n",
    "    13: ([1, 2, 3], [4])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8987e-510c-43d1-88b5-533ff5af9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts, _ = pickle.load(open(f\"{TEST_DATA_PICKLE_NAME}\", \"rb\"))\n",
    "test_prompts[list(test_prompts.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8ae1b-8535-46ea-8c06-b6fb3b6e036f",
   "metadata": {},
   "source": [
    "## Get embeddings and build lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a871c18-c647-4790-936d-531875faf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = pd.read_csv(\n",
    "    f\"{EMBEDDINGS_NAME}\", compression=\"gzip\"\n",
    ")\n",
    "product_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a23dd7-5a89-4066-ac08-7a0d1d13a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_to_name = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"name\"]\n",
    ")\n",
    "product_name_to_id = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"name\")\n",
    "    .to_dict()[\"ItemId\"]\n",
    ")\n",
    "product_index_to_embedding = (\n",
    "    product_embeddings[[\"ItemId\", \"embedding\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"embedding\"]\n",
    ")\n",
    "product_index_to_embedding = {\n",
    "    k: np.array(json.loads(v)) for k, v in product_index_to_embedding.items()\n",
    "}\n",
    "product_index_to_embedding = np.array(list(product_index_to_embedding.values()))\n",
    "product_index_to_id = list(product_id_to_name.keys())\n",
    "product_id_to_index = {idx: i for i, idx in enumerate(product_index_to_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f200116-6429-443d-9f45-61f456c29460",
   "metadata": {},
   "source": [
    "## Compute test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c624e-85e9-42ca-8db9-e95336ac39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages: list[tuple[int, list[str]]] = []\n",
    "\n",
    "for session_id, prompt in test_prompts.items():\n",
    "    custom_user_message = user_message.copy()\n",
    "    custom_user_message[\"content\"] = custom_user_message[\"content\"].replace(\"{ITEMS}\", \"\\n\".join([product_id_to_name[i] for i in prompt]))\n",
    "    test_messages.append((session_id, [system_message, custom_user_message]))\n",
    "test_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a436ef-39e8-4c0c-abaa-005449af9bcd",
   "metadata": {},
   "source": [
    "# Compute completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f3687-ca80-43fa-a9da-1cfefb1a227c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "completions: list[tuple[int, str]] = []\n",
    "\n",
    "# Use async API to get parallel requests.\n",
    "# Make sure batch_size is not too high otherwise we might hit rate limits.\n",
    "async def run_completions():\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=OPENAI_KEY,\n",
    "    )\n",
    "\n",
    "    batch_size = 150\n",
    "    for i in tqdm(range(0, len(test_messages), batch_size)):\n",
    "        start_batch = i\n",
    "        end_batch = i + batch_size\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Completion batch {start_batch} - {end_batch}\")\n",
    "\n",
    "        requests = []\n",
    "        for _, messages in test_messages[start_batch:end_batch]:\n",
    "            requests.append(\n",
    "                client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    top_p=TOP_P,\n",
    "                    messages=messages,\n",
    "                )\n",
    "            )\n",
    "        responses = await asyncio.gather(*requests)\n",
    "        for (session_id, _), response in zip(test_messages[start_batch:end_batch], responses):\n",
    "            completions.append((session_id, response.choices[0].message.content))\n",
    "            \n",
    "        print(f\"Finished batch {start_batch} - {end_batch}. Took {time.perf_counter() - start_time} seconds.\")\n",
    "\n",
    "\n",
    "await run_completions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fac383-2a4c-4942-a738-551634387083",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(completions, open(f\"completions_openai_{TOTAL_MODEL_NAME}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900c7d7-6ad4-46e0-8ce0-6b06f10812c2",
   "metadata": {},
   "source": [
    "### Parse completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece951-85dc-4518-bb25-ea6625977dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_completions: list[tuple[int, list[str]]] = []\n",
    "for session_id, response in tqdm(completions):\n",
    "    parsed_response: list[str] = parse_method(response)\n",
    "    if parsed_response is None:\n",
    "        break\n",
    "    parsed_completions.append((session_id, parsed_response))\n",
    "parsed_completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf780293-7c06-447f-a169-770f17a7b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_completions = []\n",
    "for session_id, c in completions.items():\n",
    "    recs = []\n",
    "    for rec, count in c.items():\n",
    "        for _ in range(count):\n",
    "            recs.append(rec)\n",
    "    \n",
    "    parsed_completions.append((session_id, Counter(recs)))\n",
    "parsed_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f63ee-4fa6-43f6-8c04-96fc88938b2f",
   "metadata": {},
   "source": [
    "# Completed product names to global product ids\n",
    "First we try to map to the exact product name and otherwise we use embeddings to find the closest item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ff88b-38f5-4ab2-95c3-aa7aeeb3d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmappable_items: set = set()\n",
    "for session_id, items_counter in tqdm(parsed_completions):\n",
    "    items = list(items_counter.keys())\n",
    "    for item in items:\n",
    "        if item not in product_name_to_id:\n",
    "            unmappable_items.add(item)\n",
    "print(f\"No exact match for {len(unmappable_items)} items. Will use embedding based search to find closest item.\")\n",
    "len(unmappable_items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a2243-9587-4b67-87a4-79a3c5296466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "unmappable_items_embeddings: dict[str, list[float]] = {}\n",
    "unmappable_items: list[str] = list(unmappable_items)\n",
    "async def get_embeddings():\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=OPENAI_KEY,\n",
    "    )\n",
    "\n",
    "    batch_size = 2000\n",
    "    for i in tqdm(range(0, len(unmappable_items), batch_size)):\n",
    "        start_batch = i\n",
    "        end_batch = i + batch_size\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Embeddings batch {start_batch} - {end_batch}\")\n",
    "        response = await client.embeddings.create(input = unmappable_items[start_batch:end_batch], model=\"text-embedding-ada-002\")\n",
    "        for item, embedding in zip(unmappable_items[start_batch:end_batch], response.data):\n",
    "            unmappable_items_embeddings[item] = embedding.embedding\n",
    "            \n",
    "        print(f\"Finished batch {start_batch} - {end_batch}. Took {time.perf_counter() - start_time} seconds.\")\n",
    "await get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1ff26-d74d-4a8b-b7d9-535110b24676",
   "metadata": {},
   "source": [
    "Find closest actual item (with global product id) . Try to prevent duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502e91e-d863-409b-a707-ad57a7d22328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "recommendations = {}\n",
    "bug_item_list = []\n",
    "num_sessions_done = 0\n",
    "for session_id, value_counts in tqdm(parsed_completions):\n",
    "    session_item_names = [product_id_to_name[item] for item in test_prompts[session_id]]\n",
    "    session_recommendations = []\n",
    "\n",
    "    duplicate_replacements = []\n",
    "    for item_name, count in value_counts.items():\n",
    "        # If an item occurs more than once, we need its embedding to find\n",
    "        # neighbouring items.\n",
    "        # If an item is not in the catalog, we get a similar item that is in the catalog.\n",
    "        if count > 1 or item_name not in product_name_to_id:\n",
    "            # Assert that the item is in the cache, otherwise we would\n",
    "            # retrieve these embeddings from openAI again, which is slow and expensive.\n",
    "            if item_name == \"\":\n",
    "                # This always happens when item_name is an empty string, so we just\n",
    "                # create a zero embedding.\n",
    "                item_embedding = np.zeros((1, 1024 + 512))\n",
    "            else:\n",
    "                # Get item similarity using embedding\n",
    "                if item_name in product_name_to_id:\n",
    "                    item_embedding = product_index_to_embedding[product_id_to_index[product_name_to_id[item_name]]]\n",
    "                else:\n",
    "                    item_embedding = unmappable_items_embeddings[item_name]\n",
    "                if isinstance(item_embedding, str):\n",
    "                    item_embedding = json.loads(item_embedding)\n",
    "\n",
    "                item_embedding = np.array([item_embedding], dtype=np.float64)\n",
    "\n",
    "            predictions = (product_index_to_embedding @ item_embedding.T).T[0]\n",
    "\n",
    "            # Get neighbouring item(s), and extend the recommendations for this\n",
    "            # session with the neighbouring item(s).\n",
    "            top_k_item_ids_indices = predictions.argsort()[::-1][:count + TOP_K]\n",
    "            top_k_item_ids = [\n",
    "                product_index_to_id[item_index] for item_index in top_k_item_ids_indices\n",
    "            ]\n",
    "\n",
    "            # Get names of the items that are not allowed to be added.\n",
    "            already_recommended_names = [\n",
    "                product_id_to_name[item]\n",
    "                for item in session_recommendations + duplicate_replacements\n",
    "            ]\n",
    "            upcoming_recommendations = value_counts\n",
    "            disallowed_items = (\n",
    "                already_recommended_names\n",
    "                + list(upcoming_recommendations.keys())\n",
    "                + session_item_names\n",
    "            )\n",
    "\n",
    "            # Filter out disallowed items.\n",
    "            top_k_item_ids = [\n",
    "                item\n",
    "                for item in top_k_item_ids\n",
    "                if product_id_to_name[item] not in disallowed_items\n",
    "            ]\n",
    "\n",
    "            # We add the item itself if it exists.\n",
    "            item_exists: bool = item_name in product_name_to_id\n",
    "            if item_exists:\n",
    "                item_id = product_name_to_id[item_name]\n",
    "                session_recommendations.append(item_id)\n",
    "\n",
    "            # Truncate.\n",
    "            # If an item appeared `count` times, it needs `count - int(item_exists)` replacements.\n",
    "            # If the item exists, we have added it already, so we only need count - 1 replacements.\n",
    "            # If the item does not exist, we need count replacements.\n",
    "            top_k_item_ids = top_k_item_ids[: count - int(item_exists)]\n",
    "\n",
    "            duplicate_replacements.extend(top_k_item_ids)\n",
    "\n",
    "        else:\n",
    "            # Simply add the id to the list of recommendations\n",
    "            item_id = product_name_to_id[item_name]\n",
    "            session_recommendations.append(item_id)\n",
    "\n",
    "    session_recommendations.extend(duplicate_replacements)\n",
    "\n",
    "    num_sessions_done += 1\n",
    "    if random.randint(0, 100) == 50:\n",
    "        print(f\"Num sessions done: {num_sessions_done}\")\n",
    "\n",
    "    recommendations.update({session_id: session_recommendations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16e8ad-33b4-4e08-945d-beb5df013163",
   "metadata": {},
   "source": [
    "# Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbf5d8-77a4-4c08-8774-5fbfa4ea0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recommendations, open(f\"recs_openai_{TOTAL_MODEL_NAME}.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
