{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7119fd-4203-4a78-9595-ee29aebb6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "from typing import Literal\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4f4a687a154a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a33a6e-1c7c-4533-8e2b-44c2eec9ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in name of finetuned_model.\n",
    "MODEL_NAME = \"\"\n",
    "\n",
    "DATASET: Literal[\"beauty\", \"steam\"] = \"beauty\"\n",
    "\n",
    "# Name of the pickle with the test data for Beauty.\n",
    "TEST_DATA_PICKLE_NAME = f\"test_data_{DATASET}.pickle\"\n",
    "\n",
    "# Name of the embeddings DF for \n",
    "EMBEDDINGS_NAME = f\"embeddings_{DATASET}.csv.gz\"\n",
    "\n",
    "# Fill in OpenAI key\n",
    "OPENAI_KEY = \"\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "TOP_K = 20\n",
    "TEMPERATURE = 0\n",
    "TOP_P = 1.0\n",
    "\n",
    "# load model\n",
    "model = TextGenerationModel.get_tuned_model(MODEL_NAME)\n",
    "\n",
    "# define model parameters\n",
    "parameters = {\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"top_p\": TOP_P,\n",
    "}\n",
    "\n",
    "# Correspond to respectively 4.1 to 4.4\n",
    "VARIANT: Literal[\"genitem\", \"genlist\", \"class\", \"rank\"] = \"genlist\"\n",
    "\n",
    "TOTAL_MODEL_NAME = f\"{MODEL_NAME}_{VARIANT}_temp_{TEMPERATURE}_top_p_{TOP_P}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63dd13-b8d4-4f93-a6d5-b38f5d6d436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_completion_genlist(completion: str) -> list[str]:\n",
    "    # Use regular expression to find the second occurrence of \"BEGIN\" and \"END\"\n",
    "    begin_matches = [m.start() for m in re.finditer(r\"BEGIN\", completion)]\n",
    "    end_matches = [m.start() for m in re.finditer(r\"END\", completion)]\n",
    "\n",
    "    recommendations = []\n",
    "    if len(begin_matches) >= 2 and len(end_matches) >= 2:\n",
    "        recommendations_text = completion[\n",
    "            begin_matches[1] + len(\"BEGIN\") : end_matches[1]\n",
    "        ]\n",
    "        # Use regular expression to extract items from the recommendations text\n",
    "        items = re.findall(r\"\\d+\\.\\s(.*?)\\n\", recommendations_text)\n",
    "\n",
    "        for i, item in enumerate(items, start=1):\n",
    "            recommendations.append(item)\n",
    "    else:\n",
    "        # Alternative parse strategy.\n",
    "        # Sometimes we don't have an END token.\n",
    "        recommendations_text = completion.replace(\n",
    "            \"\"\"The recommendations are in the following\n",
    "lines, in decreasing confidence order. The recommendations are delimited by\n",
    "BEGIN and END. Each recommendation is in a separate line:\n",
    "BEGIN\"\"\",\n",
    "            \"\",\n",
    "        )\n",
    "        # Use regular expression to extract items from the recommendations text\n",
    "        items = re.findall(r\"\\d+\\.\\s(.*?)\\n\", recommendations_text)\n",
    "        for i, item in enumerate(items, start=1):\n",
    "            recommendations.append(item)\n",
    "\n",
    "        if len(recommendations) == 0:\n",
    "            # Alternative parse strategy.\n",
    "            # Sometimes we don't have an END token and only newlines per product.\n",
    "            recommendations = completion.split(\"\\n\")\n",
    "    # Remove empty strings\n",
    "    recommendations = [x for x in recommendations if x]\n",
    "    return recommendations[:TOP_K]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc903f4e-c479-4e6f-bdda-f118f0e9d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a recommender system assistant.\\nProvide 20 unique item recommendations complementary to the user's item list, ordered by the confidence level of each recommendation.\\nEnsure all recommendations are from items included in the data you are fine-tuned with. List only the item names.\\n\",\n",
    "}\n",
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"The user's item list are in the following lines\\ndelimited by BEGIN and END. Each item is in a separate line:\\nBEGIN\\n{ITEMS}\\nEND\\n\",\n",
    "}\n",
    "parse_method = parse_completion_genlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b2d9b-7889-43ec-ac27-735abb77d5eb",
   "metadata": {},
   "source": [
    "## Load test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8987e-510c-43d1-88b5-533ff5af9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts, _ = pickle.load(open(f\"{TEST_DATA_PICKLE_NAME}\", \"rb\"))\n",
    "test_prompts[list(test_prompts.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8ae1b-8535-46ea-8c06-b6fb3b6e036f",
   "metadata": {},
   "source": [
    "## Get embeddings and build lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a871c18-c647-4790-936d-531875faf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = pd.read_csv(\n",
    "    f\"{EMBEDDINGS_NAME}\", compression=\"gzip\"\n",
    ")\n",
    "product_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a23dd7-5a89-4066-ac08-7a0d1d13a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_to_name = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"name\"]\n",
    ")\n",
    "product_name_to_id = (\n",
    "    product_embeddings[[\"ItemId\", \"name\"]]\n",
    "    .set_index(\"name\")\n",
    "    .to_dict()[\"ItemId\"]\n",
    ")\n",
    "product_index_to_embedding = (\n",
    "    product_embeddings[[\"ItemId\", \"embedding\"]]\n",
    "    .set_index(\"ItemId\")\n",
    "    .to_dict()[\"embedding\"]\n",
    ")\n",
    "product_index_to_embedding = {\n",
    "    k: np.array(json.loads(v)) for k, v in product_index_to_embedding.items()\n",
    "}\n",
    "product_index_to_embedding = np.array(list(product_index_to_embedding.values()))\n",
    "product_index_to_id = list(product_id_to_name.keys())\n",
    "product_id_to_index = {idx: i for i, idx in enumerate(product_index_to_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f200116-6429-443d-9f45-61f456c29460",
   "metadata": {},
   "source": [
    "## Compute test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c624e-85e9-42ca-8db9-e95336ac39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages: list[tuple[int, list[str]]] = []\n",
    "\n",
    "for session_id, prompt in test_prompts.items():\n",
    "    custom_user_message = user_message.copy()\n",
    "    custom_user_message[\"content\"] = custom_user_message[\"content\"].replace(\"{ITEMS}\", \"\\n\".join([product_id_to_name[i] for i in prompt]))\n",
    "    test_messages.append((session_id, [system_message, custom_user_message]))\n",
    "test_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd688f20371632",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vertexai_prompts = []\n",
    "\n",
    "for session_id, prompt in test_messages:\n",
    "    vertexai_prompts.append((session_id, f'{prompt[0][\"content\"]}\\n{prompt[1][\"content\"]}'))\n",
    "vertexai_prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a436ef-39e8-4c0c-abaa-005449af9bcd",
   "metadata": {},
   "source": [
    "# Compute completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f3687-ca80-43fa-a9da-1cfefb1a227c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "completions: list[tuple[int, str]] = []\n",
    "\n",
    "# Use async API to get parallel requests.\n",
    "# Make sure batch_size is not too high otherwise we might hit rate limits.\n",
    "async def run_completions():\n",
    "    batch_size = 250\n",
    "    for i in tqdm(range(0, len(vertexai_prompts), batch_size)):\n",
    "        start_batch = i\n",
    "        end_batch = i + batch_size\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Completion batch {start_batch} - {end_batch}\")\n",
    "\n",
    "        requests = []\n",
    "        for _, messages in vertexai_prompts[start_batch:end_batch]:\n",
    "            requests.append(\n",
    "                model.predict_async(\n",
    "                    prompt=messages,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    top_p=TOP_P,\n",
    "                    max_output_tokens=50*20,\n",
    "                )\n",
    "            )\n",
    "        responses = await asyncio.gather(*requests)\n",
    "        for (session_id, _), response in zip(vertexai_prompts[start_batch:end_batch], responses):\n",
    "            completions.append((session_id, response.candidates[0].text))\n",
    "            \n",
    "        print(f\"Finished batch {start_batch} - {end_batch}. Took {time.perf_counter() - start_time} seconds.\")\n",
    "\n",
    "\n",
    "await run_completions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fac383-2a4c-4942-a738-551634387083",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(completions, open(f\"completions_vertexai_{TOTAL_MODEL_NAME}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900c7d7-6ad4-46e0-8ce0-6b06f10812c2",
   "metadata": {},
   "source": [
    "### Parse completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece951-85dc-4518-bb25-ea6625977dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_completions: list[tuple[int, list[str]]] = []\n",
    "for session_id, response in tqdm(completions):\n",
    "    parsed_response: list[str] = parse_method(response)\n",
    "    if parsed_response is None:\n",
    "        break\n",
    "    parsed_completions.append((session_id, parsed_response))\n",
    "parsed_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f63ee-4fa6-43f6-8c04-96fc88938b2f",
   "metadata": {},
   "source": [
    "# Completed product names to global product ids\n",
    "First we try to map to the exact product name and otherwise we use embeddings to find the closest item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ff88b-38f5-4ab2-95c3-aa7aeeb3d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations: dict[int, list[int | str]] = {}\n",
    "unmappable_items: set = set()\n",
    "for session_id, items in tqdm(parsed_completions):\n",
    "    recommendations[session_id] = []\n",
    "    for item in items:\n",
    "        # We either transform the product name to its id if we have an exact match.\n",
    "        # Otherwise we keep the product_name (instead of an id) and find the corresponding closest id later on.\n",
    "        if item in product_name_to_id:\n",
    "            recommendations[session_id].append(product_name_to_id[item])\n",
    "        else:\n",
    "            recommendations[session_id].append(item)\n",
    "            unmappable_items.add(item)\n",
    "print(f\"No exact match for {len(unmappable_items)} items. Will use embedding based search to find closest item.\")\n",
    "len(recommendations), recommendations[list(recommendations.keys())[0]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a2243-9587-4b67-87a4-79a3c5296466",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmappable_items_embeddings: dict[str, list[float]] = {}\n",
    "unmappable_items: list[str] = list(unmappable_items)\n",
    "async def get_embeddings():\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=OPENAI_KEY,\n",
    "    )\n",
    "\n",
    "    batch_size = 2000\n",
    "    for i in tqdm(range(0, len(unmappable_items), batch_size)):\n",
    "        start_batch = i\n",
    "        end_batch = i + batch_size\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Embeddings batch {start_batch} - {end_batch}\")\n",
    "        response = await client.embeddings.create(input = unmappable_items[start_batch:end_batch], model=\"text-embedding-ada-002\")\n",
    "        for item, embedding in zip(unmappable_items[start_batch:end_batch], response.data):\n",
    "            unmappable_items_embeddings[item] = embedding.embedding\n",
    "            \n",
    "        print(f\"Finished batch {start_batch} - {end_batch}. Took {time.perf_counter() - start_time} seconds.\")\n",
    "await get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1ff26-d74d-4a8b-b7d9-535110b24676",
   "metadata": {},
   "source": [
    "Find closest actual item (with global product id) . Try to prevent duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502e91e-d863-409b-a707-ad57a7d22328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_recommendations: dict[int, list[int]] = {}\n",
    "for session_id, recs in tqdm(recommendations.items()):\n",
    "    # The unique recommendations are the ones for which we already have a global product id.\n",
    "    unique_recs: set = set([r for r in recs if isinstance(r, int)])\n",
    "    new_recs: list[int] = []\n",
    "    for r in recs:\n",
    "        if isinstance(r, int):\n",
    "            new_recs.append(r)\n",
    "            continue\n",
    "\n",
    "        item_embedding = unmappable_items_embeddings[r]\n",
    "        item_embedding = np.array([item_embedding], dtype=np.float64)\n",
    "\n",
    "        # Dot product is the same as cosine similarity of embeddings with length 1.\n",
    "        predictions = (product_index_to_embedding @ item_embedding.T).T[0]\n",
    "\n",
    "        for nearest_neighbor in predictions.argsort()[::-1]:\n",
    "            global_product_id = product_index_to_id[nearest_neighbor]\n",
    "            if global_product_id in unique_recs:\n",
    "                #print(f\"Had to continue because item {global_product_id} was already in recommendation slate.\")\n",
    "                continue\n",
    "            else:\n",
    "                #print(f\"Matching '{r}' to '{product_id_to_name[global_product_id]}' with confidence '{predictions[nearest_neighbor]}'\")\n",
    "                new_recs.append(global_product_id)\n",
    "                unique_recs.add(global_product_id)\n",
    "                break\n",
    "\n",
    "\n",
    "    final_recommendations[session_id] = new_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16e8ad-33b4-4e08-945d-beb5df013163",
   "metadata": {},
   "source": [
    "# Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbf5d8-77a4-4c08-8774-5fbfa4ea0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recommendations, open(f\"recs_vertexai_{TOTAL_MODEL_NAME}.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
